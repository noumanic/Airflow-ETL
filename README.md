# Dependency Chain (Linear Pipeline)
extract_data
     â†“
transform_data
     â†“
load_data
     â†“
generate_report
     â†“
send_email


# Dependency Explanation (Task by Task)
ğŸ”¹ 1. extract_data

Upstream: None

Downstream: transform_data

Reason: Raw data must be extracted first.

ğŸ”¹ 2. transform_data

Upstream: extract_data

Downstream: load_data

Reason: Data transformation requires extracted data.

ğŸ”¹ 3. load_data

Upstream: transform_data

Downstream: generate_report

Reason: Data must be loaded before reporting.

ğŸ”¹ 4. generate_report

Upstream: load_data

Downstream: send_email

Reason: Report is generated only after load completion.

ğŸ”¹ 5. send_email

Upstream: generate_report

Downstream: None

Reason: Email is sent after report generation.

# How Dependencies Are Defined in Code (VERY IMPORTANT)
ğŸ”¹ Implicit Dependencies (TaskFlow API)
raw = extract_data()
transformed = transform_data(raw)
loaded = load_data(transformed)


ğŸ“Œ What happens internally

Airflow creates dependencies automatically

Return values are passed using XCom

Execution order is enforced

ğŸ“Œ Exam Line

In TaskFlow API, dependencies are inferred from function calls.

ğŸ”¹ Explicit Dependencies (Bitshift Operator)
loaded >> report_task >> email_task


Equivalent to:

loaded.set_downstream(report_task)
report_task.set_downstream(email_task)


ğŸ“Œ Used for classic operators.

6ï¸âƒ£ Dependency Type Used in This DAG
Type	Used?	Explanation
Implicit	âœ… Yes	TaskFlow return-based
Explicit	âœ… Yes	>> operator
Conditional	âŒ No	No branching
Parallel	âŒ No	Linear pipeline
7ï¸âƒ£ What Happens If a Task Fails?
Failed Task	Result
extract_data	All downstream blocked
transform_data	Load, report, email skipped
load_data	Report & email skipped
generate_report	Email skipped

ğŸ“Œ Exam phrase

Airflow enforces fail-fast execution using dependency constraints.



# Automatic Data Passing (VERY IMPORTANT)
How data flows without XCom code:
python
Copy code
raw = extract_data()
transformed = transform_data(raw)
ğŸ“Œ Why this works

TaskFlow API automatically pushes return values to XCom

Next task receives data as function arguments

ğŸ“Œ Exam Line

TaskFlow API simplifies XCom by passing data through function returns.


# How to Run This Project
Step 1: Initialize Airflow
docker-compose up airflow-webserver airflow-scheduler

Step 2: Open UI
http://localhost:8080


Username: airflow

Password: airflow

Step 3: Trigger DAG

Enable etl_taskflow_pipeline

Click â–¶ Run


Q: Why TaskFlow API?

It simplifies DAG creation and enables automatic data passing.

Q: Why docker-compose?

To orchestrate multiple Airflow services reliably.

Q: How tasks communicate?

Using XCom via function return values.

Q: Difference between PythonOperator and @task?

@task is higher-level and cleaner abstraction.




# Executor and the Schedular
Yes, both the Scheduler and Executor are used â€” even though we did not write code for them explicitly.
They are configured and run at the infrastructure level, not inside the DAG code.

ğŸ”¹ Why You Didnâ€™t â€œSeeâ€ Them in the Code

In Airflow:

DAG code â†’ defines what should run

Scheduler â†’ decides when tasks run

Executor â†’ decides how & where tasks run

ğŸ‘‰ Scheduler and Executor are Airflow services, not Python functions.

ğŸ”¹ Where the Scheduler Is Used (Very Important)

In your setup:

airflow-scheduler:
  command: scheduler


ğŸ“Œ This container IS the scheduler.

What it does internally:

Parses DAG files from dags/

Checks schedules (@daily)

Resolves dependencies

Sends tasks to the executor

ğŸ“Œ Exam line

The Airflow scheduler continuously monitors DAGs and triggers task instances when their conditions are met.

ğŸ”¹ Where the Executor Is Used

In docker-compose.yml:

AIRFLOW__CORE__EXECUTOR: LocalExecutor


ğŸ“Œ This tells Airflow:

â€œExecute tasks using the LocalExecutorâ€

What LocalExecutor does:

Runs tasks in parallel

Uses local processes

Suitable for development & small production

ğŸ“Œ Exam line

The executor determines how tasks are executed and scaled.

ğŸ”¹ Complete Flow (THIS IS GOLD FOR VIVA)
DAG Code
   â†“
Scheduler detects runnable tasks
   â†“
Executor executes the tasks
   â†“
Worker process runs Python code


ğŸ“Œ Key insight

DAGs do NOT execute themselves â€” the scheduler + executor do.

ğŸ”¹ Why You Should NOT Use Scheduler/Executor in DAG Code

âŒ WRONG:

scheduler = Scheduler()
executor = LocalExecutor()


ğŸ“Œ Reason:

Airflow is declarative

Execution is managed by Airflow services

Mixing execution logic in DAGs breaks scalability

ğŸ“Œ Exam phrase

Airflow separates workflow definition from execution.

ğŸ”¹ Did We Use Them? (Direct Answer)
Component	Used?	Where
Scheduler	âœ… Yes	airflow-scheduler service
Executor	âœ… Yes	AIRFLOW__CORE__EXECUTOR
Webserver	âœ… Yes	UI & monitoring
Metadata DB	âœ… Yes	PostgreSQL
ğŸ”¹ How to Explicitly Change Executor (Exam Tip)
Example: CeleryExecutor
AIRFLOW__CORE__EXECUTOR: CeleryExecutor


ğŸ“Œ Requires:

Redis / RabbitMQ

Worker containers